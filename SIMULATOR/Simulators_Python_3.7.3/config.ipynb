{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb569568",
   "metadata": {},
   "source": [
    "# Universal Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Number of virtual processors\n",
    "max_processors = 100\n",
    "processor_amount = max_processors\n",
    "\n",
    "# Number of clients\n",
    "client_amount = 3\n",
    "\n",
    "# Minimum split\n",
    "min_split = client_amount\n",
    "\n",
    "# Enter the max number of possible paths to be calculated by each allocation algorithm\n",
    "max_paths = 25\n",
    "\n",
    "# Enter the number of events you want to simulate\n",
    "event_amount = 20\n",
    "\n",
    "# Enter the number of epochs you wish to train your NN with\n",
    "epochs = 1\n",
    "\n",
    "# Set the layers for your DNN\n",
    "# NN Mini\n",
    "max_layers = 9\n",
    "# NN STD\n",
    "# max_layers = 12\n",
    "\n",
    "# Size of an individual sample in your batch\n",
    "# Enter the data for your batch \n",
    "\n",
    "# NN Mini\n",
    "batch_individual_file_size = 0.000503909138323 # GB\n",
    "\n",
    "# NN STD\n",
    "# batch_individual_file_size = 0.000810277642072 # GB\n",
    "\n",
    "# Multiple\n",
    "# batch_individual_file_size = 0.001 # GB\n",
    "batch_size = 100 # Amount of files in your batch\n",
    "total_batch_size = batch_size * batch_individual_file_size\n",
    "\n",
    "# UPDATE G_base with the value of your batch_size. G_base is the complexity for ONE sample of\n",
    "# ONE FP, but now in each FP you have BATCH_SIZE amount of samples in ONE FP, hence:\n",
    "G_base = G_base * batch_size\n",
    "\n",
    "# In case you are training with different sizes of data, set your total # of batches\n",
    "# for each architecture. \n",
    "#Total batches for: C&D = 75, ECHO = 124, MNIST = 100\n",
    "total_batches_FL = total_batches_SL = total_batches_PSL = total_batches_FSL = 100\n",
    "total_batches_PL = total_batches_FL * client_amount\n",
    "\n",
    "# Set the variation % (plus-minus this %) for the processing power and Bandiwdth between\n",
    "# each event\n",
    "C_variation = 0.4\n",
    "B_variation = 0.4\n",
    "\n",
    "# In case your distributed learning implementation does NOT use the same parallelization approach\n",
    "# as the measurement tool for power, use this factor to easily adjust for this difference\n",
    "# For example, PyTorch may by default use all cores, while PySyft might not. \n",
    "# Default = 1.0 = no change\n",
    "correction_factor_MINI = 1.0\n",
    "correction_factor_STD = 1.0\n",
    "\n",
    "# In case you use a parallelization strategy in your system that is not accounted by the simulator\n",
    "# you can use this correction factor to decrease the predicted time. For example, if you already know\n",
    "# your strategy decreases processing execution times across the board by 10%, then use 0.9 \n",
    "correction_factor_parall = 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
