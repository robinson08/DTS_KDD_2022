{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad417f6b",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1338f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FLOP measurements\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49161cfa",
   "metadata": {},
   "source": [
    "# General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2564378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size Y where Y represents YxY \n",
    "IMG_SIZE = 50\n",
    "BATCH_SIZE = 124\n",
    "LR = 0.001\n",
    "\n",
    "# Set the memory safety_factor, to run on the cautious side and not crashing because of random\n",
    "# system memory spikes because of external processes\n",
    "MEM_SF = 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dbaeee",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- From here on you are free to uncomment all print()'s if you wish to see more details\n",
    "- If not, just modify your NN classes, then RUN ALL CELLS and check your results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0768581",
   "metadata": {},
   "source": [
    "# Complete CNN\n",
    "Define your complete CNN, as if it were running all on a single processor, in the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a052ec",
   "metadata": {},
   "source": [
    "## NN Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3ba776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        IMG_SIZE = 50\n",
    "        # Define your first convolutional layer: input = 1, output = 32 convolutional features, kernel size = 5\n",
    "        # Remember that kernel = 5 means that the \"window\" used to scan for features will be 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "#         self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "        \n",
    "        \n",
    "        # Now we need to get the output of the convolution processed to get it into a fully connected layer\n",
    "        # To know what dimensions to use for our fully connected layers, the only known way in PyTorch is to \n",
    "        # pass some data through the convolutional layers, and then examine their output. Then we manually \n",
    "        # input the dimensions of this output into the next fully connected layer and voila.\n",
    "        \n",
    "        # Create random dummy data, run the convolution layers, check the size of the output of conv3, and then \n",
    "        # give that dimension info to the fully connected layers. This will only be done ONCE\n",
    "        x = torch.randn(IMG_SIZE,IMG_SIZE).view(-1, 1, IMG_SIZE, IMG_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        # Run the fully connected layers\n",
    "#         self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "#         self.fc2 = nn.Linear(512, 2)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "#         self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    # Function defining only one part of the forward pass (the convolution layers only). This will also write\n",
    "    # the output dimensions of the conv layers to self._to_linear ONCE, and this information will then be used \n",
    "    # as the input data flattened dimensions of the next fully connected layers \n",
    "    def convs(self, x):\n",
    "        # Convolutional layer 1 + activation + max_pooling\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "#         x = self.conv4(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        \n",
    "#         print(\"x[0].shape = \", x[0].shape)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    # Function defining the rest of the forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convs layers first\n",
    "        x = self.convs(x)\n",
    "        # Reshape the output data from the convs to be flattened\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        # Pass the data through the fully connected layers now\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        # Pass it through the final layer\n",
    "        x = self.fc2(x)\n",
    "        # One final softmax function to make the output vector look nicer\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4c31a",
   "metadata": {},
   "source": [
    "## NN Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002efd9d",
   "metadata": {},
   "source": [
    "Take a look at your CNN and obtain some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932edf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at your model\n",
    "model = net\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced6fa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in this model:  72802\n"
     ]
    }
   ],
   "source": [
    "# Obtain the FLOPs of one FP and the total size of the model in memory\n",
    "mac_count, param_count = get_model_complexity_info(net, (1, IMG_SIZE, IMG_SIZE), as_strings=False,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "# print('Computational complexity: ', mac_count, \" MACs\")\n",
    "\n",
    "# Remember that 1 MAC = 1 Multiply-Accumulation operation = 2 FLOPs, so:\n",
    "# print('Computational complexity: ', mac_count * 2, \" FLOPs\")\n",
    "# And one 1 GFLOP = 1000000000 FLOP so:\n",
    "G_base = mac_count * 2 / 1000000000\n",
    "# print('Computational complexity: ', G_base, \" GFLOPs\")\n",
    "# print('Number of parameters: ', param_count)\n",
    "\n",
    "# Convert these: Params --> Bytes --> kB --> MB --> GB\n",
    "# Remember your weights are 32 bit floats, and each 32 bit float == 4 bytes\n",
    "# We  will need an ADDITIONAL triple of this amount because of the backwards pass\n",
    "# BP includes: activations and gradients for neurons, gradients for weights, momentum, etc.\n",
    "# And we will need this amount of parameters for each image in the batch, so:\n",
    "M_base = (param_count * 4 * 1/1024 * 1/1024 * 1/1024) * (1 + 3) * BATCH_SIZE * MEM_SF\n",
    "# print(\"Total size in memory:  \", round(M_base, 5), \" GB\")\n",
    "print(\"Total parameters in this model: \", param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56601f",
   "metadata": {},
   "source": [
    "# Split NN Mini\n",
    "Specify your CNN split into Server and Client models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed14272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_client(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define your first convolutional layer: input = 1, output = 32 convolutional features, kernel size = 5\n",
    "        # Remember that kernel = 5 means that the \"window\" used to scan for features will be 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "\n",
    "    # Function defining only one part of the forward pass (the convolution layers only). This will also write\n",
    "    # the output dimensions of the conv layers to self._to_linear ONCE, and this information will then be used \n",
    "    # as the input data flattened dimensions of the next fully connected layers \n",
    "    def convs(self, x):\n",
    "        # Convolutional layer 1 + activation + max_pooling\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        return x\n",
    "    \n",
    "    # Function defining the rest of the forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convs layers first\n",
    "        x = self.convs(x)\n",
    "        return x\n",
    "\n",
    "net_client = Net_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd088e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_server(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Start from the third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "        \n",
    "        # Run the fully connected layers. We know the input of this fc1 layer is 512, because of our previous\n",
    "        # results with FL, where self.__to__linear told us this result when you run the cell that contains the \n",
    "        # NN\n",
    "        self._to_linear = 256\n",
    "        self.fc1 = nn.Linear(self._to_linear, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    # Function defining only one part of the forward pass (the convolution layers only). This will also write\n",
    "    # the output dimensions of the conv layers to self._to_linear ONCE, and this information will then be used \n",
    "    # as the input data flattened dimensions of the next fully connected layers \n",
    "    def convs(self, x):\n",
    "        # Convolutional layer 1 + activation + max_pooling\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "#         return x\n",
    "    \n",
    "    # Function defining the rest of the forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convs layers first\n",
    "        x = self.convs(x)\n",
    "        # Reshape the output data from the convs to be flattened\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        # Pass the data through the fully connected layers now\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass it through the final layer\n",
    "        x = self.fc2(x)\n",
    "        # One final softmax function to make the output vector look nicer\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "net_server = Net_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a580e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at our models\n",
    "model_client = net_client\n",
    "model_server = net_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89489d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_client(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at your model\n",
    "model_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4ab7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for this model\n",
    "# Obtain the FLOPs of one FP\n",
    "mac_count_client, param_count_client = get_model_complexity_info(Net_client(), (1, IMG_SIZE, IMG_SIZE), as_strings=False,\n",
    "                                           print_per_layer_stat=False, verbose=False)\n",
    "# print('Computational complexity: ', mac_count_client, \" MACs\")\n",
    "\n",
    "# Remember that 1 MAC = 1 Multiply-Accumulation operation = 2 FLOPs, so:\n",
    "# print('Computational complexity: ', mac_count_client * 2, \" FLOPs\")\n",
    "# And one 1 GFLOP = 1000000000 FLOP so:\n",
    "G_client = mac_count_client * 2 / 1000000000\n",
    "# print('Computational complexity: ', G_client, \" GFLOPs\")\n",
    "# print('Number of parameters: ', param_count_client)\n",
    "\n",
    "# Convert these: Params --> Bytes --> kB --> MB --> GB\n",
    "# Remember your weights are 32 bit floats, and each 32 bit float == 4 bytes\n",
    "# We  will need an ADDITIONAL triple of this amount because of the backwards pass\n",
    "# BP includes: activations and gradients for neurons, gradients for weights, momentum, etc.\n",
    "# And we will need this amount of parameters for each image in the batch, so:\n",
    "M_client = (param_count_client * 4 * 1/1024 * 1/1024 * 1/1024) * (1 + 3) * BATCH_SIZE * MEM_SF\n",
    "# print(\"Total size in memory:  \", round(M_client, 5), \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466358b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_server(\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at your model\n",
    "model_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3092e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for this model\n",
    "# Because we already know the results for the whole NN and for the client side, obtaining the server side results\n",
    "# is simple arithmetic\n",
    "mac_count_server = mac_count - mac_count_client\n",
    "param_count_server = param_count - param_count_client\n",
    "G_server = G_base - G_client\n",
    "M_server = M_base - M_client\n",
    "\n",
    "# print('Computational complexity: ', mac_count_server, \" MACs\")\n",
    "# Remember that 1 MAC = 1 Multiply-Accumulation operation = 2 FLOPs, so:\n",
    "# print('Computational complexity: ', mac_count_server * 2, \" FLOPs\")\n",
    "G_server = mac_count_server * 2 / 1000000000\n",
    "# print('Computational complexity: ', G_server, \" GFLOPs\")\n",
    "# print('Number of parameters: ', param_count_server)\n",
    "\n",
    "# Convert these: Params --> Bytes --> kB --> MB --> GB\n",
    "# Remember your weights are 32 bit floats, and each 32 bit float == 4 bytes\n",
    "# We  will need an ADDITIONAL triple of this amount because of the backwards pass\n",
    "# BP includes: activations and gradients for neurons, gradients for weights, momentum, etc.\n",
    "# And we will need this amount of parameters for each image in the batch, so:\n",
    "M_server = (param_count_server * 4 * 1/1024 * 1/1024 * 1/1024) * (1 + 3) * BATCH_SIZE * MEM_SF\n",
    "# print(\"Total size in memory:  \", round(M_server, 5), \" GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae96d02",
   "metadata": {},
   "source": [
    "# Measurement Results\n",
    "The following results are shown with the same variable names as the one used for the simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb154cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model =  72802  parameters \n",
      "\n",
      "G_base =  0.013604996  GFLOP\n",
      "G_server =  0.00257978  GFLOP\n",
      "G_client =  0.011025216  GFLOP\n",
      "G_agg =  1.3604996e-05  GFLOP \n",
      "\n",
      "M_base =  0.17487529516220093  GB\n",
      "M_server =  0.14305270910263063  GB\n",
      "M_client =  0.031822586059570314  GB\n",
      "M_agg =  0.17487529516220093  GB \n",
      "\n",
      "D_weights =  0.17487529516220093  GB\n",
      "D_client_out =  0.031822586059570314  GB\n"
     ]
    }
   ],
   "source": [
    "# Model's total parameters:\n",
    "print(\"Total parameters in model = \", param_count, \" parameters \\n\")\n",
    "\n",
    "# print(\"Total FLOPs for one FP of ONE SAMPLE (NOT one batch, NOT one epoch): \")\n",
    "print(\"G_base = \", G_base, \" GFLOP\")\n",
    "# print(\"FLOPs done for one FP on the server side:\")\n",
    "print(\"G_server = \", G_server, \" GFLOP\")\n",
    "# print(\"FLOPs done for one FP on the client side:\")\n",
    "print(\"G_client = \", G_client, \" GFLOP\")\n",
    "# print(\"FLOPs needed for averaged and aggregating the models (in specific architectures): \")\n",
    "print(\"G_agg = \", G_base * 0.001, \" GFLOP \\n\")\n",
    "\n",
    "\n",
    "# print(\"Total GB in local memory needed for the model:\")\n",
    "print(\"M_base = \", M_base, \" GB\")\n",
    "# print(\"GBs in local memory needed for the model on the server:\")\n",
    "print(\"M_server = \", M_server, \" GB\")\n",
    "# print(\"GBs in local memory needed for the model on the clients:\")\n",
    "print(\"M_client = \", M_client, \" GB\")\n",
    "# print(\"GBs in local memory needed for the aggregation phase on the server:\")\n",
    "print(\"M_agg = \", M_base, \" GB \\n\") # Because we federated averaged ALL parameters in this phase!\n",
    "\n",
    "\n",
    "# print(\"Size of weights to be sent to server for federated averaging:\") #ALL parameters!\n",
    "print(\"D_weights = \", M_base, \" GB\")\n",
    "# print(\"Size of data to be sent as intermediate results from client to server:\") # Size of ALL client params sent!\n",
    "print(\"D_client_out = \", M_client, \" GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
