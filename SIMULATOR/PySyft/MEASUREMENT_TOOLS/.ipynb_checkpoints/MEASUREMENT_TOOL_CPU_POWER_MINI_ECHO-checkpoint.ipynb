{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d95381",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e375295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f5098",
   "metadata": {},
   "source": [
    "# General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115979d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size Y where Y represents YxY \n",
    "IMG_SIZE = 50\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6755fac",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- From here on you are free to uncomment all print()'s if you wish to see more details\n",
    "- If not, just modify your NN classes, then RUN ALL CELLS and check your results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fef665",
   "metadata": {},
   "source": [
    "# Complete CNN\n",
    "Define your complete CNN, as if it were running all on a single processor, in the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee1dfd",
   "metadata": {},
   "source": [
    "## NN STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00411c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        IMG_SIZE = 50\n",
    "        # Define your first convolutional layer: input = 1, output = 32 convolutional features, kernel size = 5\n",
    "        # Remember that kernel = 5 means that the \"window\" used to scan for features will be 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)\n",
    "#         self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "        \n",
    "        \n",
    "        # Now we need to get the output of the convolution processed to get it into a fully connected layer\n",
    "        # To know what dimensions to use for our fully connected layers, the only known way in PyTorch is to \n",
    "        # pass some data through the convolutional layers, and then examine their output. Then we manually \n",
    "        # input the dimensions of this output into the next fully connected layer and voila.\n",
    "        \n",
    "        # Create random dummy data, run the convolution layers, check the size of the output of conv3, and then \n",
    "        # give that dimension info to the fully connected layers. This will only be done ONCE\n",
    "        x = torch.randn(IMG_SIZE,IMG_SIZE).view(-1, 1, IMG_SIZE, IMG_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        # Run the fully connected layers\n",
    "#         self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "#         self.fc2 = nn.Linear(512, 2)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "#         self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    # Function defining only one part of the forward pass (the convolution layers only). This will also write\n",
    "    # the output dimensions of the conv layers to self._to_linear ONCE, and this information will then be used \n",
    "    # as the input data flattened dimensions of the next fully connected layers \n",
    "    def convs(self, x):\n",
    "        # Convolutional layer 1 + activation + max_pooling\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "#         x = self.conv4(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        \n",
    "#         print(\"x[0].shape = \", x[0].shape)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    # Function defining the rest of the forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convs layers first\n",
    "        x = self.convs(x)\n",
    "        # Reshape the output data from the convs to be flattened\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        # Pass the data through the fully connected layers now\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        # Pass it through the final layer\n",
    "        x = self.fc2(x)\n",
    "        # One final softmax function to make the output vector look nicer\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6545de2",
   "metadata": {},
   "source": [
    "Take a look at your CNN and obtain some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ca9162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at your model\n",
    "model_complete = net\n",
    "model_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8dc74d",
   "metadata": {},
   "source": [
    "# Obtain the data to use for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddf11309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file it was saved in\n",
    "training_data = np.load(r\"/media/wilfredo/Willie931GB/SLU/EchoNet/EchoNet-Dynamic/Numpy_Datasets/training_data_size_50.npy\", allow_pickle = True)\n",
    "test_data = np.load(r\"/media/wilfredo/Willie931GB/SLU/EchoNet/EchoNet-Dynamic/Numpy_Datasets/test_data_size_50.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd700d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## TRAIN DATA ##########################\n",
    "# Separate the x's and the y's\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 1, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# Scale the images. The pixel values are between 0-255, but we want them to be between 0-1\n",
    "X = X/255.0\n",
    "\n",
    "# Get your y's\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "########################## TEST DATA ##########################\n",
    "# Separate the x's and the y's\n",
    "X_test = torch.Tensor([i[0] for i in test_data]).view(-1, 1, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# Scale the images. The pixel values are between 0-255, but we want them to be between 0-1\n",
    "X_test = X_test/255.0\n",
    "\n",
    "# Get your y's\n",
    "y_test = torch.Tensor([i[1] for i in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a693c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training data\n",
    "train_X = X\n",
    "train_y = y\n",
    "\n",
    "# Define your testing (validation) data\n",
    "test_X = X_test\n",
    "test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f6c3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your loss function (MSE for images!)\n",
    "loss_function = nn.MSELoss()\n",
    "# Set your optimizer\n",
    "optimizer = optim.SGD(model_complete.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24f34",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. Run the FP on one epoch\n",
    "2. Record the time it takes for FP AND BP for every batch\n",
    "3. Save results and average them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a9d61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [01:00<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_FP_time PER BATCH =  0.0555  s\n",
      "Your processor power for these operations is:  24.516663732863027  GFLOP/s\n"
     ]
    }
   ],
   "source": [
    "FP_time_list = []\n",
    "BP_time_list = []\n",
    "for i in tqdm(range(0, int(len(train_X)), BATCH_SIZE)):\n",
    "        \n",
    "    # Get the batches for each of the workers\n",
    "    batch_X = train_X[i : i + BATCH_SIZE]\n",
    "    batch_y = train_y[i : i + BATCH_SIZE]\n",
    "\n",
    "    # Train the models\n",
    "    # The following in real life would be done in parallel\n",
    "    FP_start_time = time.time()\n",
    "    pred = model_complete(batch_X)\n",
    "    FP_end_time = time.time() - FP_start_time\n",
    "\n",
    "    # Calculate the loss functions\n",
    "    BP_start_time = time.time()\n",
    "    loss = loss_function(pred, batch_y)\n",
    "\n",
    "    # Do the backwards pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    BP_end_time = time.time() - BP_start_time\n",
    "\n",
    "    # Accumulate the times\n",
    "    FP_time_list.append(FP_end_time)\n",
    "    BP_time_list.append(BP_end_time)\n",
    "        \n",
    "# OUTSIDE THE FOR LOOP: After all workers are done training...\n",
    "avg_FP_time = sum(FP_time_list)/len(FP_time_list)\n",
    "avg_BP_time = sum(BP_time_list)/len(BP_time_list)\n",
    "print(\"avg_FP_time PER BATCH = \", round(avg_FP_time, 4), \" s\")\n",
    "# print(\"avg_BP_time PER BATCH = \", round(avg_BP_time, 4), \" s \\n\")\n",
    "\n",
    "nn_size = 0.013604996 # GFLOP PER **IMAGE**\n",
    "nn_size_batch = nn_size * BATCH_SIZE # GFLOP PER **BATCH** OF IMAGES\n",
    "# Now we have GFLOP/batch and SECONDS/batch, so:\n",
    "proc_power = nn_size_batch / avg_FP_time # turns to GFLOP/s\n",
    "print(\"Your processor power for these operations is: \", proc_power, \" GFLOP/s\")\n",
    "\n",
    "# Clean the global namespace\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
