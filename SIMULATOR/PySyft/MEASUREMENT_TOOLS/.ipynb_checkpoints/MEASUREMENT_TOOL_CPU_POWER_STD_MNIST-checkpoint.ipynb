{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b5c8fd",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e239f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf09172",
   "metadata": {},
   "source": [
    "# General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ceca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size Y where Y represents YxY \n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c898fa",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- From here on you are free to uncomment all print()'s if you wish to see more details\n",
    "- If not, just modify your NN classes, then RUN ALL CELLS and check your results at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d09289",
   "metadata": {},
   "source": [
    "# Complete CNN\n",
    "Define your complete CNN, as if it were running all on a single processor, in the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f69432",
   "metadata": {},
   "source": [
    "## NN STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eeac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        IMG_SIZE = 64\n",
    "        # Define your first convolutional layer: input = 1, output = 32 convolutional features, kernel size = 5\n",
    "        # Remember that kernel = 5 means that the \"window\" used to scan for features will be 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "        \n",
    "        \n",
    "        # Now we need to get the output of the convolution processed to get it into a fully connected layer\n",
    "        # To know what dimensions to use for our fully connected layers, the only known way in PyTorch is to \n",
    "        # pass some data through the convolutional layers, and then examine their output. Then we manually \n",
    "        # input the dimensions of this output into the next fully connected layer and voila.\n",
    "        \n",
    "        # Create random dummy data, run the convolution layers, check the size of the output of conv3, and then \n",
    "        # give that dimension info to the fully connected layers. This will only be done ONCE\n",
    "        x = torch.randn(IMG_SIZE,IMG_SIZE).view(-1, 1, IMG_SIZE, IMG_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        # Run the fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    # Function defining only one part of the forward pass (the convolution layers only). This will also write\n",
    "    # the output dimensions of the conv layers to self._to_linear ONCE, and this information will then be used \n",
    "    # as the input data flattened dimensions of the next fully connected layers \n",
    "    def convs(self, x):\n",
    "        # Convolutional layer 1 + activation + max_pooling\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        \n",
    "#         print(\"x[0].shape = \", x[0].shape)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    # Function defining the rest of the forward pass\n",
    "    def forward(self, x):\n",
    "        # Run the convs layers first\n",
    "        x = self.convs(x)\n",
    "        # Reshape the output data from the convs to be flattened\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        # Pass the data through the fully connected layers now\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Pass it through the final layer\n",
    "        x = self.fc3(x)\n",
    "        # One final softmax function to make the output vector look nicer\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109e368",
   "metadata": {},
   "source": [
    "Take a look at your CNN and obtain some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09268af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at your model\n",
    "model_complete = net\n",
    "model_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f186a8",
   "metadata": {},
   "source": [
    "# Obtain the data to use for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9fa2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(r\"/media/wilfredo/Willie931GB/EURECOM_SLU_Linux/II_SEMESTER/SLU/PAPER_KDD2022/EXPERIMENTS/PySyft/Datasets/MNIST_64\", \n",
    "                      train = True, download = True, \n",
    "                      transform = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                                      transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(r\"/media/wilfredo/Willie931GB/EURECOM_SLU_Linux/II_SEMESTER/SLU/PAPER_KDD2022/EXPERIMENTS/PySyft/Datasets/MNIST_64\", \n",
    "                      train = False, download = True, \n",
    "                      transform = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                                      transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fab01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file it was saved in. Take the ENTIRE dataset!\n",
    "training_data = torch.utils.data.DataLoader(train, batch_size = int(len(train)/2), shuffle = True)\n",
    "test_data = torch.utils.data.DataLoader(test, batch_size = int(len(test)/2), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d36f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the data loaded onto training_data. You NEED to iterate over it to take it, even if you\n",
    "# want to take the entire thing. Make sure to convert the values to floats\n",
    "X = next(iter(training_data))[0]\n",
    "y_unformatted = next(iter(training_data))[1].type(torch.FloatTensor)\n",
    "X_test = next(iter(test_data))[0]\n",
    "y_test_unformatted = next(iter(test_data))[1].type(torch.FloatTensor)\n",
    "\n",
    "# The two other cases in this paper use 2 dimensional labels (0, 1), not only (0)\n",
    "# MNIST by default comes with labels in the format (9) instead of (9, 0). To change this:\n",
    "# Create tensors with all zeros of the same size\n",
    "y_unformatted_addition = torch.zeros(y_unformatted.size())\n",
    "y_test_unformatted_addition = torch.zeros(y_test_unformatted.size())\n",
    "# Then stack them together (0 for vertically, -1 for horizontally)\n",
    "y = torch.stack((y_unformatted, y_unformatted_addition), -1)\n",
    "y_test = torch.stack((y_test_unformatted, y_test_unformatted_addition), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39bccfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training data\n",
    "train_X = X\n",
    "train_y = y\n",
    "\n",
    "# Define your testing (validation) data\n",
    "test_X = X_test\n",
    "test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a8c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your loss function (MSE for images!)\n",
    "loss_function = nn.MSELoss()\n",
    "# Set your optimizer\n",
    "optimizer = optim.SGD(model_complete.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47377c",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "1. Run the FP on one epoch\n",
    "2. Record the time it takes for FP AND BP for every batch\n",
    "3. Save results and average them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6edb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:05<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_FP_time PER BATCH =  0.2129  s\n",
      "Your processor power for these operations is:  30.81119543152012  GFLOP/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FP_time_list = []\n",
    "BP_time_list = []\n",
    "for i in tqdm(range(0, int(len(train_X)), BATCH_SIZE)):\n",
    "        \n",
    "    # Get the batches for each of the workers\n",
    "    batch_X = train_X[i : i + BATCH_SIZE]\n",
    "    batch_y = train_y[i : i + BATCH_SIZE]\n",
    "\n",
    "    # Train the models\n",
    "    # The following in real life would be done in parallel\n",
    "    FP_start_time = time.time()\n",
    "    pred = model_complete(batch_X)\n",
    "    FP_end_time = time.time() - FP_start_time\n",
    "\n",
    "    # Calculate the loss functions\n",
    "    BP_start_time = time.time()\n",
    "    loss = loss_function(pred, batch_y)\n",
    "\n",
    "    # Do the backwards pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    BP_end_time = time.time() - BP_start_time\n",
    "\n",
    "    # Accumulate the times\n",
    "    FP_time_list.append(FP_end_time)\n",
    "    BP_time_list.append(BP_end_time)\n",
    "        \n",
    "# OUTSIDE THE FOR LOOP: After all workers are done training...\n",
    "avg_FP_time = sum(FP_time_list)/len(FP_time_list)\n",
    "avg_BP_time = sum(BP_time_list)/len(BP_time_list)\n",
    "print(\"avg_FP_time PER BATCH = \", round(avg_FP_time, 4), \" s\")\n",
    "# print(\"avg_BP_time PER BATCH = \", round(avg_BP_time, 4), \" s \\n\")\n",
    "\n",
    "nn_size = 0.06559962 # GFLOP PER **IMAGE**\n",
    "nn_size_batch = nn_size * BATCH_SIZE # GFLOP PER **BATCH** OF IMAGES\n",
    "# Now we have GFLOP/batch and SECONDS/batch, so:\n",
    "proc_power = nn_size_batch / avg_FP_time # turns to GFLOP/s\n",
    "print(\"Your processor power for these operations is: \", proc_power, \" GFLOP/s\")\n",
    "\n",
    "# Clean the global namespace\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
